{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJcYs_ERTnnI"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HMUDt0CiUJk9"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77z2OchJTk0l"
      },
      "source": [
        "# Migrating from TPU embedding_columns (with TPUEstimator) to TPUEmbedding layer (TF2)\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/migrate/tpu_embedding\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/migrate/tpu_embedding.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/migrate/tpu_embedding.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/migrate/tpu_embedding.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meUTrR4I6m1C"
      },
      "source": [
        "In TF1, `tf.compat.v1.estimator.tpu.TPUEstimator` is a high level API that encapsulates training, evaluation, prediction, and export for serving with TPUs. It has special support for `tf.compat.v1.tpu.experimental.embedding_column`, which is a TPU based embedding feature column which allows you to use embedding tables that are larger than memory of a single TPU device or to use sparse or ragged input to embeddings on the TPU. In TF2, use `tf.keras` with a Keras [model](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly), [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer?version=nightly), [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer?version=nightly), and [metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) for the aforementioned tasks. For Keras and TPUs we have produced a special `tfrs.layers.embedding.TPUEmbedding` layer that performs the same functions as the TPU `embedding_column`s with `TPUEstimator`. In this guide, you will go through the basic setup of a `TPUEstimator` training and evaluation program to run it with an `embedding_column`. Then, you will create the equivalent training and evaluation program in TF2 using Keras and the `TPUEmbedding` layer, with the help of `tf.distribute.TPUStrategy`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdZSoIXEbhg-"
      },
      "source": [
        "## Setup\n",
        "\n",
        "For both TF1 and TF2 examples demonstrated below, you will first start with a couple of necessary TensorFlow imports.\n",
        "\n",
        "**Note**: `tf-nightly` and `cloud_tpu_client` are only necessary here until TensorFlow 2.6 is available in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYE3RnRN2jNu"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-recommenders\n",
        "!pip uninstall -y tensorflow keras tensorflow-estimator\n",
        "!pip install tf-nightly\n",
        "!pip install cloud_tpu_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE0vSfMXumKI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "\n",
        "# TPUEmbedding layer is not part of Tensorflow\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "# Import the CloudTPU client to set the runtime version to match the current tensorflow version\n",
        "try:\n",
        "  from cloud_tpu_client import Client\n",
        "  c = Client(tpu='')\n",
        "  c.configure_tpu_version(tf.__version__, restart_type='ifNeeded')\n",
        "except ImportError:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsm9Rxx7s1OZ"
      },
      "source": [
        "and prepare some simple data for demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7rnGxsXtDkV"
      },
      "outputs": [],
      "source": [
        "features = [[1., 1.5]]\n",
        "embedding_features_indices = [[0, 0], [0, 1]]\n",
        "embedding_features_values = [0, 5]\n",
        "labels = [[0.3]]\n",
        "eval_features = [[4., 4.5]]\n",
        "eval_embedding_features_indices = [[0, 0], [0, 1]]\n",
        "eval_embedding_features_values = [4, 3]\n",
        "eval_labels = [[0.8]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uXff1BEssdE"
      },
      "source": [
        "## TF1: TPUEstimator.train/evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc-WSeYG2oje"
      },
      "source": [
        "When using TPU embeddings with `TPUEstimator`, we will need to define a feature column for each feature we want to lookup and then pass that information to the `TPUEstimator` via the `embedding_config_spec` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO_y-IRT3dcM"
      },
      "outputs": [],
      "source": [
        "# Create the embedding column, the key must match one of the keys in the dict of input features.\n",
        "# The num_buckets is the vocabular size for the embedding table.\n",
        "embedding_id_column = (\n",
        "      tf1.feature_column.categorical_column_with_identity(\n",
        "          key=\"sparse_feature\", num_buckets=10))\n",
        "\n",
        "# dimension is the width of the embedding table.\n",
        "embedding_column = tf1.tpu.experimental.embedding_column(\n",
        "    embedding_id_column, dimension=5)\n",
        "\n",
        "embedding_config_spec = tf1.estimator.tpu.experimental.EmbeddingConfigSpec(\n",
        "    feature_columns=(embedding_column,),\n",
        "    optimization_parameters=(\n",
        "        tf1.tpu.experimental.AdagradParameters(0.05)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVWHEQj5a7rN"
      },
      "source": [
        "Next, to use a `TPUEstimator`, you should define a few functions: an input function for the training data, an evaluation input function for the evaluation data, and a model function that tells the `TPUEstimator` how the training op is defined with the features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqe9obf7suIj"
      },
      "outputs": [],
      "source": [
        "def _input_fn(params):\n",
        "  dataset = tf1.data.Dataset.from_tensor_slices((\n",
        "      {\"dense_feature\": features,\n",
        "       \"sparse_feature\": tf1.SparseTensor(\n",
        "           embedding_features_indices,\n",
        "           embedding_features_values, [1, 2])},\n",
        "           labels))\n",
        "  dataset = dataset.repeat()\n",
        "  return dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "\n",
        "def _eval_input_fn(params):\n",
        "  dataset = tf1.data.Dataset.from_tensor_slices((\n",
        "      {\"dense_feature\": eval_features,\n",
        "       \"sparse_feature\": tf1.SparseTensor(\n",
        "           eval_embedding_features_indices,\n",
        "           eval_embedding_features_values, [1, 2])},\n",
        "           eval_labels))\n",
        "  dataset = dataset.repeat()\n",
        "  return dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "\n",
        "def _model_fn(features, labels, mode, params):\n",
        "  embedding_features = tf1.keras.layers.DenseFeatures(embedding_column)(features)\n",
        "  concatenated_features = tf1.keras.layers.Concatenate(axis=1)(\n",
        "      [embedding_features, features[\"dense_feature\"]])\n",
        "  logits = tf1.layers.Dense(1)(concatenated_features)\n",
        "  loss = tf1.losses.mean_squared_error(labels=labels, predictions=logits)\n",
        "  optimizer = tf1.train.AdagradOptimizer(0.05)\n",
        "  optimizer = tf1.tpu.CrossShardOptimizer(optimizer)\n",
        "  train_op = optimizer.minimize(loss, global_step=tf1.train.get_global_step())\n",
        "  return tf1.estimator.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYnP3Dszc-2R"
      },
      "source": [
        "With those functions defined, you will now create a `tf.distribute.cluster_resolver.TPUClusterResolver` that provides the cluster information, and a `tf.compat.v1.estimator.tpu.RunConfig` object. Using the model function you have defined, you can now create a `TPUEstimator`. Checkpoint saving is skipped to simplify the example. Note that you need to specify the batch size for both training and evaluation for a `TPUEstimator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAqyqawemlcl"
      },
      "outputs": [],
      "source": [
        "cluster_resolver = tf1.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "print(\"All devices: \", tf1.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsOpjW5plH9Q"
      },
      "outputs": [],
      "source": [
        "tpu_config = tf1.estimator.tpu.TPUConfig(\n",
        "    iterations_per_loop=10,\n",
        "    per_host_input_for_training=tf1.estimator.tpu.InputPipelineConfig\n",
        "          .PER_HOST_V2)\n",
        "config = tf1.estimator.tpu.RunConfig(\n",
        "    cluster=cluster_resolver,\n",
        "    save_checkpoints_steps=None,\n",
        "    tpu_config=tpu_config)\n",
        "estimator = tf1.estimator.tpu.TPUEstimator(\n",
        "    model_fn=_model_fn, config=config, train_batch_size=8, eval_batch_size=8,\n",
        "    embedding_config_spec=embedding_config_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxw7tWrcepaZ"
      },
      "source": [
        "Call `train` to train the `TPUEstimator`,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZPKFOMAcyrP"
      },
      "outputs": [],
      "source": [
        "estimator.train(_input_fn, steps=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev1vjIz9euIw"
      },
      "source": [
        "And call `evaluate` to evaluate the model using the evaluation data you have prepared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqiKRiwWc0cz"
      },
      "outputs": [],
      "source": [
        "estimator.evaluate(_eval_input_fn, steps=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEmzBjfnsxwT"
      },
      "source": [
        "### TF2: Keras training API with `tf.distribute.TPUStrategy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UesuXNbShrbi"
      },
      "source": [
        "In TF2, you will use a `tf.keras` model with a `tf.distribute.TPUStrategy` to train your models on the TPU workers. To begin with, you will create a `TPUClusterResolver` to provide the cluster information, and connect to the cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TgdPNgXoS63"
      },
      "outputs": [],
      "source": [
        "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94JBD0HxmdPI"
      },
      "source": [
        "Next, prepare your data. This is similar to how you created a dataset in TensorFlow 1, except the dataset function is now passed a `tf.distribute.InputContext` object  rather than a params dict. You can use this object to determine the local batch size (and which host this pipeline is for, so you can properly partition your data).\n",
        "\n",
        "When using TPU Embeddings it is important to include the 'drop_remainder=True' option when batching since the TPU Embedding API required a fixed batch size. More over the same batch size must be used to evaluation and training if they are taking place on the same set of devices.\n",
        "\n",
        "Finally, you should use `tf.keras.utils.experimental.DatasetCreator` along with the special input option `experimental_fetch_to_device=False` as seen below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NTruOw6mcy9"
      },
      "outputs": [],
      "source": [
        "global_batch_size = 8\n",
        "\n",
        "def _input_dataset(context: tf.distribute.InputContext):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      {\"dense_feature\": features,\n",
        "       \"sparse_feature\": tf.SparseTensor(\n",
        "           embedding_features_indices,\n",
        "           embedding_features_values, [1, 2])},\n",
        "           labels))\n",
        "  dataset = dataset.shuffle(10).repeat()\n",
        "  dataset = dataset.batch(\n",
        "      context.get_per_replica_batch_size(global_batch_size),\n",
        "      drop_remainder=True)\n",
        "  return dataset.prefetch(2)\n",
        "\n",
        "def _eval_dataset(context: tf.distribute.InputContext):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      {\"dense_feature\": eval_features,\n",
        "       \"sparse_feature\": tf.SparseTensor(\n",
        "           eval_embedding_features_indices,\n",
        "           eval_embedding_features_values, [1, 2])},\n",
        "           eval_labels))\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(\n",
        "      context.get_per_replica_batch_size(global_batch_size),\n",
        "      drop_remainder=True)\n",
        "  return dataset.prefetch(2)\n",
        "\n",
        "input_options = tf.distribute.InputOptions(\n",
        "    experimental_fetch_to_device=False)\n",
        "\n",
        "input_dataset = tf.keras.utils.experimental.DatasetCreator(\n",
        "    _input_dataset, input_options=input_options)\n",
        "\n",
        "eval_dataset = tf.keras.utils.experimental.DatasetCreator(\n",
        "    _eval_dataset, input_options=input_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4EHXhN3CVmo"
      },
      "source": [
        "Next, once your data is prepared, you will create a `TPUStrategy`, and define your model, metrics, and optimizer under the scope of this strategy. You should pick a number for `steps_per_execution` in `Model.compile` because it specifies the number of batches to run during each `tf.function` call, and is critical for performance. This argument is similar to `iterations_per_loop` used in `TPUEstimator`.\n",
        "\n",
        "The features and table configuration that was previously specified via the `tf1.tpu.experimental.embedding_column` and `tf1.tpu.experimental.shared_embedding_column` are now specified directly via a pair of configuration objects `tf.tpu.experimental.embedding.FeatureConfig` and `tf.tpu.experimental.embedding.TableConfig`. Please see the associated documentation for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atVciNgPs0fw"
      },
      "outputs": [],
      "source": [
        "strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "with strategy.scope():\n",
        "  optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)\n",
        "  dense_input = tf.keras.Input(shape=(2,), dtype=tf.float32, batch_size=global_batch_size)\n",
        "  sparse_input = tf.keras.Input(shape=(), dtype=tf.int32, batch_size=global_batch_size)\n",
        "  embedded_input = tfrs.layers.embedding.TPUEmbedding(\n",
        "      feature_config=tf.tpu.experimental.embedding.FeatureConfig(\n",
        "          table=tf.tpu.experimental.embedding.TableConfig(\n",
        "              vocabulary_size=10,\n",
        "              dim=5,\n",
        "              initializer=tf.initializers.TruncatedNormal(mean=0.0, stddev=1)),\n",
        "          name=\"sparse_input\"),\n",
        "      optimizer=optimizer)(sparse_input)\n",
        "  input = tf.keras.layers.Concatenate(axis=1)([dense_input, embedded_input])\n",
        "  result = tf.keras.layers.Dense(1)(input)\n",
        "  model = tf.keras.Model(inputs={\"dense_feature\": dense_input, \"sparse_feature\": sparse_input}, outputs=result)\n",
        "  model.compile(optimizer, \"mse\", steps_per_execution=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkM2VZyni98F"
      },
      "source": [
        "With that, you are ready to train the model with the training dataset,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kip65sYBlKiu"
      },
      "outputs": [],
      "source": [
        "model.fit(input_dataset, epochs=5, steps_per_epoch=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0AEK8sNjLOj"
      },
      "source": [
        "and evaluate the model using the evaluation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tMRkyfKhqSL"
      },
      "outputs": [],
      "source": [
        "model.evaluate(eval_dataset, steps=1, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHx_RUL8xcJ3"
      },
      "source": [
        "For more customization such as optimization procedure, Keras APIs allow you to define the training step. Please see [Customize what happens in Model.fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
        ") for an example of this workflow, and [Use TPUs](https://www.tensorflow.org/guide/tpu) for more information of TPU usage, including an example of customized training loop."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tpu_embedding.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
